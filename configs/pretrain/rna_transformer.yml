task_name: rna_transformer_pretrain

train:
  # for ditributed training
  local_rank: 0
  # random seed
  seed: 2002

  # training hyperparameters
  batch_size: 26
  lr: 1e-4
  epochs: 40
  gradient_accumulation_steps: 20
  amp: True
  save_ckpt_freq: 2

  model:
    encoder: transformer
    pretrained: null # set to None if not using pretrained model
    embedding_method: categeoried
    max_seq_len: 8000
    embedding_dim: 512
    num_layers: 6
    head_num: 8
    head_dim: 1024
    dropout: 0.2
    additional_config_path: /path/to/additional_config.json
    cell_emb_style: cls

valid:
  freq: 2

data:
  bin_num: &bn 52
  include_zero: False
  append_cls: True
  train:
    mask_ratio: 0.12
    path: /home/jwu418/workspace/data/ours/train/bmmc_rna_binned.h5ad
    key: X_binned
  test:
    mask_ratio: 0.12
    path: /home/jwu418/workspace/data/ours/valid/bmmc_rna_binned.h5ad
    key: X_binned

vocab:
  path: /home/jwu418/workspace/data/ours/vocab/bmmc_rna_binned.json
  cell_type_path: /home/jwu418/workspace/data/ours/vocab/bmmc_cell_vocab.json
  special_tokens:
    pad: {token: <pad>, value: 52}
    # value of the mask is 1 plus bin_num
    mask: {token: <mask>, value: 53}
    cls: {token: <cls>, value: 0}